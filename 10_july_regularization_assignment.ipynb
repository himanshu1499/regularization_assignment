{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8a51bee-230d-4b01-98d3-30acc85eab33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# part 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acbd6b00-4040-4efa-9a59-c4b3d552f827",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31239963-2756-44dd-b215-811e73aeefba",
   "metadata": {},
   "outputs": [],
   "source": [
    "In the context of deep learning, regularization refers to a set of techniques used to prevent overfitting and improve the generalization ability of a neural network model. Overfitting occurs when a model learns to perform extremely well on the training data but fails to generalize to unseen data. Regularization helps to address this issue by introducing additional constraints or penalties on the model's parameters during the training process.\n",
    "\n",
    "Regularization is important for several reasons:\n",
    "\n",
    "1. Preventing Overfitting: The primary goal of regularization is to prevent overfitting. Deep learning models are highly flexible and capable of memorizing the training data, but they may struggle to generalize well to new, unseen data. Regularization techniques introduce constraints on the model's parameters, making it less likely to fit the noise or irrelevant patterns present in the training data.\n",
    "\n",
    "2. Improving Generalization: By reducing overfitting, regularization techniques help improve the generalization ability of the model. A regularized model tends to perform better on unseen data, which is crucial for real-world applications where the ultimate goal is to make accurate predictions or classifications on new, unseen examples.\n",
    "\n",
    "3. Handling Limited Training Data:Deep learning models often require a large amount of labeled training data to perform well. However, in many practical scenarios, obtaining a vast amount of labeled data can be challenging or costly. Regularization techniques can help mitigate the issue of limited training data by effectively utilizing the available data and reducing the chances of overfitting.\n",
    "\n",
    "4. Controlling Model Complexity: Regularization provides a way to control the complexity of a deep learning model. It introduces penalties or constraints that encourage the model to have simpler patterns or parameter values, which can help prevent the model from becoming overly complex and potentially harder to train or interpret.\n",
    "\n",
    "5. Addressing Collinearity and Redundancy: Regularization techniques can handle collinearity and redundancy in the input features. When there are highly correlated features in the data, regularization can encourage the model to select the most informative and relevant features, thereby reducing noise and improving the model's performance.\n",
    "\n",
    "Some commonly used regularization techniques in deep learning include L1 regularization (Lasso), L2 regularization (Ridge), dropout, and early stopping. These techniques provide different ways to introduce constraints or penalties on the model's parameters, and their choice depends on the specific characteristics of the problem at hand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf65304d-7753-49fa-8af8-78c945ded326",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17a48413-c5de-423e-bf73-d2a1e634cb2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "The bias-variance tradeoff is a fundamental concept in machine learning, including deep learning. It refers to the tradeoff between a model's ability to fit the training data well (low bias) and its ability to generalize to new, unseen data (low variance).\n",
    "\n",
    "- **Bias**: Bias measures how much the predictions of a model deviate from the true values in the training data. A model with high bias may oversimplify the underlying patterns and make systematic errors. It typically occurs when the model is too simple or does not have enough capacity to capture the complexity of the data.\n",
    "\n",
    "- **Variance**: Variance, on the other hand, quantifies the variability of the model's predictions across different training sets. A model with high variance is sensitive to the specific training examples and can perform well on the training data but poorly on new data. It often occurs when the model is too complex and overfits the noise or idiosyncrasies of the training set.\n",
    "\n",
    "The goal is to find the right balance between bias and variance to achieve good generalization performance. Regularization techniques play a crucial role in addressing the bias-variance tradeoff:\n",
    "\n",
    "- **Reducing Variance**: Regularization methods such as L1 and L2 regularization can help reduce variance by adding a penalty term to the model's objective function. This penalty discourages overly complex models and encourages the model to generalize better to new data. By constraining the parameter values, regularization can prevent overfitting and improve the model's ability to generalize.\n",
    "\n",
    "- **Controlling Model Complexity**: Regularization techniques provide a way to control the complexity of the model. By adding constraints or penalties on the model's parameters, regularization encourages the model to prefer simpler patterns and avoid overfitting. It helps prevent the model from becoming overly flexible and reduces the variance.\n",
    "\n",
    "- **Handling Noise and Irrelevant Features**: Regularization can effectively handle noisy or irrelevant features in the data. By applying penalties on the parameters, regularization encourages the model to assign lower weights or eliminate unnecessary features. This reduces the impact of noise and irrelevant factors, improving the model's robustness and generalization.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47648988-f46a-4e0e-ab4b-314e7a0f55f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7977ac5e-a696-432f-beda-8b59815f7ba1",
   "metadata": {},
   "outputs": [],
   "source": [
    " They are two popular regularization techniques used in machine learning and deep learning to address overfitting and improve generalization. Both methods introduce penalties on the model's parameters, but they differ in terms of how the penalties are calculated and their effects on the model.\n",
    "\n",
    "L1 Regularization (Lasso):\n",
    "\n",
    "L1 regularization, also known as Lasso regularization, adds a penalty term proportional to the absolute value of the parameters' weights. The L1 penalty is calculated as the sum of the absolute values of the parameters:\n",
    "\n",
    "L1_penalty = λ * ∑|θ|,\n",
    "\n",
    "where λ is the regularization strength hyperparameter, and θ represents the model's parameters.\n",
    "\n",
    "Effects and Characteristics of L1 Regularization:\n",
    "\n",
    "L1 regularization encourages sparsity in the parameter values. It tends to drive some of the parameter weights to zero, effectively performing feature selection by identifying and excluding less important or irrelevant features.\n",
    "The sparsity induced by L1 regularization makes the model more interpretable, as it highlights the most influential features.\n",
    "L1 regularization can lead to a sparse solution, which can be useful when dealing with high-dimensional data or situations where feature selection is desired.\n",
    "Due to its sparsity-inducing nature, L1 regularization can be more robust to outliers in the data compared to L2 regularization.\n",
    "L2 Regularization (Ridge):\n",
    "\n",
    "L2 regularization, also known as Ridge regularization, adds a penalty term proportional to the squared magnitude of the parameters' weights. The L2 penalty is calculated as the sum of the squared values of the parameters:\n",
    "\n",
    "L2_penalty = λ * ∑θ^2,\n",
    "\n",
    "where λ is the regularization strength hyperparameter, and θ represents the model's parameters.\n",
    "\n",
    "Effects and Characteristics of L2 Regularization:\n",
    "\n",
    "L2 regularization encourages small parameter weights, effectively shrinking their magnitudes towards zero without necessarily setting them exactly to zero.\n",
    "L2 regularization does not induce sparsity in the parameter values like L1 regularization does. It assigns non-zero weights to all features but reduces their impact.\n",
    "L2 regularization can help prevent multicollinearity issues by spreading the impact across correlated features. It can handle situations where multiple features are informative but highly correlated.\n",
    "L2 regularization generally leads to smoother and more stable solutions compared to L1 regularization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fed42e35-16f7-4807-9e77-d4aab0d930c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eca7b0f-b9fd-4515-ac26-842f9976f0b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "Regularization plays a crucial role in preventing overfitting and improving the generalization performance of deep learning models. Overfitting occurs when a model becomes too complex and starts to memorize the noise or idiosyncrasies of the training data, resulting in poor performance on new, unseen data. Regularization techniques introduce constraints or penalties on the model's parameters, which helps address overfitting and improve generalization in the following ways:\n",
    "\n",
    "Parameter Constraint: Regularization techniques add constraints on the model's parameters, preventing them from taking excessively large values. By limiting the range of parameter values, regularization reduces the complexity of the model and its ability to fit noise or irrelevant patterns in the training data. This constraint ensures that the model focuses on the most important and relevant features for making predictions, leading to better generalization.\n",
    "\n",
    "Simplification of Model: Regularization encourages models to prefer simpler patterns and solutions. By penalizing complex models, regularization favors models that are less prone to overfitting. It helps the model avoid overemphasizing noise or peculiarities in the training data, forcing it to generalize better to unseen examples. Regularization can be seen as a tradeoff between fitting the training data well and finding a simpler, more generalizable solution.\n",
    "\n",
    "Reduction of Variance: Regularization techniques reduce the variance of the model's predictions across different training sets. High variance is a characteristic of overfitting, where the model's predictions can vary significantly depending on the specific training examples. By introducing regularization, the model becomes less sensitive to the idiosyncrasies of the training data, leading to more stable and consistent predictions on new data.\n",
    "\n",
    "Feature Selection: Certain regularization techniques, such as L1 regularization (Lasso), can induce sparsity in the model's parameters. This means that some of the parameter weights can be pushed towards zero, effectively performing feature selection. By identifying and excluding irrelevant or less important features, regularization helps the model focus on the most informative ones. This not only reduces noise but also simplifies the model, leading to improved generalization performance.\n",
    "\n",
    "Handling Limited Training Data: Deep learning models often require a large amount of labeled training data to perform well. However, in many real-world scenarios, obtaining a vast amount of labeled data can be challenging. Regularization techniques help mitigate the issue of limited training data by effectively utilizing the available data. By reducing overfitting and improving generalization, regularization allows the model to make more accurate predictions even with a smaller training dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e02688a2-1529-4ac4-9f7a-2c82a21db7bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# part 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aea33b35-76ce-402a-b1a0-50b2af3a3682",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "416e7bd5-7122-4728-b572-49e8e6c0d8cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "Dropout regularization is a widely used technique in deep learning to combat overfitting. It involves temporarily dropping out (i.e., setting to zero) a random subset of neurons during each training iteration. This technique helps in reducing complex co-adaptations between neurons, encourages the model to be more robust, and improves generalization. Dropout has a significant impact on both model training and inference.\n",
    "\n",
    "During model training:\n",
    "1. Random Neuron Dropout: In each training iteration, dropout randomly selects a subset of neurons to be dropped out. The probability of dropping out a neuron is typically defined by a hyperparameter called the dropout rate, which is usually set between 0.2 and 0.5. The selected neurons are set to zero, effectively removing their contributions from the network for that specific iteration.\n",
    "2. Ensemble Effect: Dropout can be seen as training an ensemble of multiple models within a single model. Each dropout mask applied during training results in a different configuration of the network, leading to a diverse set of subnetworks. These subnetworks collectively make predictions, effectively creating an ensemble. This ensemble effect helps in reducing overfitting and improving the model's generalization ability.\n",
    "3. Regularization Effect: Dropout acts as a regularization technique by introducing noise and reducing complex co-adaptations between neurons. The dropped out neurons force the network to learn more robust representations that are not overly dependent on specific neurons. This regularization effect prevents the model from relying too heavily on a few dominant features, making it more resilient to noise and improving generalization.\n",
    "\n",
    "During model inference:\n",
    "1. Scaling of Neuron Activations: During inference, when making predictions on new, unseen data, the dropout regularization is not applied. However, to maintain the expected activations, the outputs of the remaining neurons are scaled by the inverse of the dropout rate. This scaling ensures that the overall magnitudes of the neuron activations are preserved and comparable to the training phase.\n",
    "2. Ensemble Averaging: The ensemble effect of dropout during training can be mimicked during inference by using multiple predictions from the subnetworks created by dropout. In practice, this is achieved by performing multiple forward passes through the network with different dropout masks and averaging the predictions. Ensemble averaging helps in reducing the impact of individual subnetwork biases and provides more reliable predictions.\n",
    "\n",
    "The impact of dropout regularization on model training and inference can be summarized as follows:\n",
    "- During training, dropout reduces overfitting by randomly dropping out neurons, promoting robustness and preventing complex co-adaptations between neurons.\n",
    "- During inference, the scaling of neuron activations and ensemble averaging (if employed) help ensure consistent and reliable predictions, considering the impact of dropout during training.\n",
    "\n",
    "Overall, dropout regularization is an effective technique for reducing overfitting, improving generalization, and enhancing the robustness of deep learning models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d66cf7e2-a0f3-4e8c-b534-42a5949e4975",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3b5123b-7284-41a9-9224-7dc826ba6131",
   "metadata": {},
   "outputs": [],
   "source": [
    "Early stopping is a regularization technique used to prevent overfitting during the training process of a machine learning model. It involves monitoring the model's performance on a validation set and stopping the training when the performance starts to degrade, rather than continuing until the model completely fits the training data.\n",
    "\n",
    "The concept of early stopping can be understood as follows:\n",
    "\n",
    "1. **Training Progress**: During model training, the performance of the model is typically evaluated using a validation set, which consists of data that the model has not seen during training. The performance metric, such as validation loss or accuracy, is monitored at regular intervals.\n",
    "\n",
    "2. **Monitoring Performance**: As the training progresses, the model's performance on the validation set is continuously evaluated. Initially, the performance tends to improve, indicating that the model is learning and generalizing well. However, after a certain point, the performance on the validation set may start to deteriorate.\n",
    "\n",
    "3. **Early Stopping Criteria**: Early stopping is based on a predefined criterion, such as an increase in the validation loss or a decrease in validation accuracy. Once the criterion is met, the training process is stopped, and the model's parameters at that point are considered the final model.\n",
    "\n",
    "4. **Preventing Overfitting**: Early stopping helps prevent overfitting by stopping the training process before the model becomes overly specialized to the training data. By monitoring the validation performance, early stopping identifies the point at which the model starts to memorize the noise or idiosyncrasies of the training set and fails to generalize well to new data. This prevents the model from reaching its full capacity to fit the training data while sacrificing its ability to generalize.\n",
    "\n",
    "The key idea behind early stopping is that there exists an optimal point in the training process where the model achieves the best tradeoff between fitting the training data and generalizing to unseen data. Continuing the training beyond this point leads to overfitting. Early stopping helps in identifying this optimal point by monitoring the model's performance on a separate validation set.\n",
    "\n",
    "It is worth noting that early stopping should be used with caution and is subject to finding the right balance. Stopping the training too early may result in an underfit model that does not fully capture the patterns in the data. On the other hand, stopping too late may lead to overfitting. The determination of when to stop the training process is often based on experimentation, considering the behavior of the model's performance on the validation set over multiple training iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c04152ab-258c-44a2-bea4-860ef0499c8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ad06633-623d-479a-a2d2-8d914cc7730a",
   "metadata": {},
   "outputs": [],
   "source": [
    "Batch Normalization is a technique used in deep learning to normalize the intermediate activations within a mini-batch during training. It helps in addressing the internal covariate shift problem and acts as a form of regularization. Batch Normalization plays a role in preventing overfitting through the following mechanisms:\n",
    "\n",
    "Normalization: Batch Normalization normalizes the activations of each layer by subtracting the batch mean and dividing by the batch standard deviation. This normalization step helps in reducing the internal covariate shift, which refers to the change in the distribution of layer inputs as the parameters of preceding layers change during training. By normalizing the inputs, Batch Normalization helps stabilize the learning process and enables the network to learn more efficiently.\n",
    "\n",
    "Regularization Effect: Batch Normalization introduces a regularization effect by adding noise to the network. During each mini-batch, the batch mean and standard deviation are estimated based on the batch statistics, which are subject to some degree of noise. This noise acts as a form of regularization and helps in reducing overfitting. It makes the model more robust by reducing the reliance on specific instances or examples within a mini-batch and encourages the network to learn more generalizable representations.\n",
    "\n",
    "Reduced Internal Covariate Shift: By normalizing the activations within each mini-batch, Batch Normalization reduces the internal covariate shift. This shift in the distribution of layer inputs can make the training process more challenging. By maintaining more stable distributions of inputs, Batch Normalization helps in achieving faster convergence and smoother optimization, making the training process more effective and efficient.\n",
    "\n",
    "Effect on Learning Rate: Batch Normalization can influence the effective learning rate of the model. By normalizing the inputs, it reduces the dependence of the network on the scale of the weights and biases. This allows the learning rate to be set higher, enabling faster convergence and avoiding the issue of vanishing or exploding gradients. The ability to use higher learning rates can help in finding better minima during optimization and improve generalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4fa6c32-05b4-4c0d-8bcf-1cb8dbcbd339",
   "metadata": {},
   "outputs": [],
   "source": [
    "# part 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f691a3f-7058-4919-9dd1-3882b69754aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a20005c6-555e-4bb8-a74e-e307729b633f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow\n",
      "  Downloading tensorflow-2.13.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (524.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m524.1/524.1 MB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: typing-extensions<4.6.0,>=3.6.6 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (4.4.0)\n",
      "Collecting wrapt>=1.11.0\n",
      "  Downloading wrapt-1.15.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (78 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.4/78.4 kB\u001b[0m \u001b[31m13.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting absl-py>=1.0.0\n",
      "  Downloading absl_py-1.4.0-py3-none-any.whl (126 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m126.5/126.5 kB\u001b[0m \u001b[31m21.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting grpcio<2.0,>=1.24.3\n",
      "  Downloading grpcio-1.56.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.2/5.2 MB\u001b[0m \u001b[31m78.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting flatbuffers>=23.1.21\n",
      "  Downloading flatbuffers-23.5.26-py2.py3-none-any.whl (26 kB)\n",
      "Requirement already satisfied: numpy<=1.24.3,>=1.22 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (1.23.4)\n",
      "Collecting termcolor>=1.1.0\n",
      "  Downloading termcolor-2.3.0-py3-none-any.whl (6.9 kB)\n",
      "Collecting astunparse>=1.6.0\n",
      "  Downloading astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from tensorflow) (65.5.0)\n",
      "Requirement already satisfied: six>=1.12.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (1.16.0)\n",
      "Collecting libclang>=13.0.0\n",
      "  Downloading libclang-16.0.0-py2.py3-none-manylinux2010_x86_64.whl (22.9 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m22.9/22.9 MB\u001b[0m \u001b[31m58.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting keras<2.14,>=2.13.1\n",
      "  Downloading keras-2.13.1-py3-none-any.whl (1.7 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m73.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (4.21.8)\n",
      "Collecting gast<=0.4.0,>=0.2.1\n",
      "  Downloading gast-0.4.0-py3-none-any.whl (9.8 kB)\n",
      "Collecting tensorflow-io-gcs-filesystem>=0.23.1\n",
      "  Downloading tensorflow_io_gcs_filesystem-0.32.0-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (2.4 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m67.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hCollecting tensorflow-estimator<2.14,>=2.13.0\n",
      "  Downloading tensorflow_estimator-2.13.0-py2.py3-none-any.whl (440 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m440.8/440.8 kB\u001b[0m \u001b[31m47.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting tensorboard<2.14,>=2.13\n",
      "  Downloading tensorboard-2.13.0-py3-none-any.whl (5.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m79.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting google-pasta>=0.1.1\n",
      "  Downloading google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.5/57.5 kB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting opt-einsum>=2.3.2\n",
      "  Downloading opt_einsum-3.3.0-py3-none-any.whl (65 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.5/65.5 kB\u001b[0m \u001b[31m12.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from tensorflow) (21.3)\n",
      "Requirement already satisfied: h5py>=2.9.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (3.7.0)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /opt/conda/lib/python3.10/site-packages (from astunparse>=1.6.0->tensorflow) (0.37.1)\n",
      "Collecting tensorboard-data-server<0.8.0,>=0.7.0\n",
      "  Downloading tensorboard_data_server-0.7.1-py3-none-manylinux2014_x86_64.whl (6.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.6/6.6 MB\u001b[0m \u001b[31m74.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting werkzeug>=1.0.1\n",
      "  Downloading Werkzeug-2.3.6-py3-none-any.whl (242 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m242.5/242.5 kB\u001b[0m \u001b[31m28.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: requests<3,>=2.21.0 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.14,>=2.13->tensorflow) (2.28.1)\n",
      "Collecting google-auth<3,>=1.6.3\n",
      "  Downloading google_auth-2.21.0-py2.py3-none-any.whl (182 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m182.1/182.1 kB\u001b[0m \u001b[31m23.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting google-auth-oauthlib<1.1,>=0.5\n",
      "  Downloading google_auth_oauthlib-1.0.0-py2.py3-none-any.whl (18 kB)\n",
      "Collecting markdown>=2.6.8\n",
      "  Downloading Markdown-3.4.3-py3-none-any.whl (93 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m93.9/93.9 kB\u001b[0m \u001b[31m16.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->tensorflow) (3.0.9)\n",
      "Collecting pyasn1-modules>=0.2.1\n",
      "  Downloading pyasn1_modules-0.3.0-py2.py3-none-any.whl (181 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m181.3/181.3 kB\u001b[0m \u001b[31m26.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: urllib3<2.0 in /opt/conda/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow) (1.26.11)\n",
      "Collecting cachetools<6.0,>=2.0.0\n",
      "  Downloading cachetools-5.3.1-py3-none-any.whl (9.3 kB)\n",
      "Collecting rsa<5,>=3.1.4\n",
      "  Downloading rsa-4.9-py3-none-any.whl (34 kB)\n",
      "Collecting requests-oauthlib>=0.7.0\n",
      "  Downloading requests_oauthlib-1.3.1-py2.py3-none-any.whl (23 kB)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow) (3.4)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow) (2.1.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow) (2022.9.24)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /opt/conda/lib/python3.10/site-packages (from werkzeug>=1.0.1->tensorboard<2.14,>=2.13->tensorflow) (2.1.1)\n",
      "Collecting pyasn1<0.6.0,>=0.4.6\n",
      "  Downloading pyasn1-0.5.0-py2.py3-none-any.whl (83 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m83.9/83.9 kB\u001b[0m \u001b[31m13.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: oauthlib>=3.0.0 in /opt/conda/lib/python3.10/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.14,>=2.13->tensorflow) (3.2.2)\n",
      "Installing collected packages: libclang, flatbuffers, wrapt, werkzeug, termcolor, tensorflow-io-gcs-filesystem, tensorflow-estimator, tensorboard-data-server, pyasn1, opt-einsum, markdown, keras, grpcio, google-pasta, gast, cachetools, astunparse, absl-py, rsa, requests-oauthlib, pyasn1-modules, google-auth, google-auth-oauthlib, tensorboard, tensorflow\n",
      "Successfully installed absl-py-1.4.0 astunparse-1.6.3 cachetools-5.3.1 flatbuffers-23.5.26 gast-0.4.0 google-auth-2.21.0 google-auth-oauthlib-1.0.0 google-pasta-0.2.0 grpcio-1.56.0 keras-2.13.1 libclang-16.0.0 markdown-3.4.3 opt-einsum-3.3.0 pyasn1-0.5.0 pyasn1-modules-0.3.0 requests-oauthlib-1.3.1 rsa-4.9 tensorboard-2.13.0 tensorboard-data-server-0.7.1 tensorflow-2.13.0 tensorflow-estimator-2.13.0 tensorflow-io-gcs-filesystem-0.32.0 termcolor-2.3.0 werkzeug-2.3.6 wrapt-1.15.0\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7270fec9-8099-449e-b47f-e4d2af4dea4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "housing = fetch_california_housing()\n",
    "housing\n",
    "\n",
    "\n",
    "X = pd.DataFrame(housing.data, columns= housing.feature_names)\n",
    "X.head()\n",
    "\n",
    "y = pd.DataFrame(housing.target, columns=['target'])\n",
    "y.head()\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(X,y, random_state=42)\n",
    "x_train, x_valid, y_train, y_valid = train_test_split(x_train,y_train, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "16308954-0c81-4310-bc0c-3f85a4433060",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "363/363 [==============================] - 1s 2ms/step - loss: -131433.6250 - accuracy: 0.0029\n",
      "Epoch 2/10\n",
      "363/363 [==============================] - 0s 1ms/step - loss: -2300027.5000 - accuracy: 0.0029\n",
      "Epoch 3/10\n",
      "363/363 [==============================] - 1s 1ms/step - loss: -10078787.0000 - accuracy: 0.0029\n",
      "Epoch 4/10\n",
      "363/363 [==============================] - 0s 1ms/step - loss: -26135418.0000 - accuracy: 0.0029\n",
      "Epoch 5/10\n",
      "363/363 [==============================] - 0s 1ms/step - loss: -52526100.0000 - accuracy: 0.0029\n",
      "Epoch 6/10\n",
      "363/363 [==============================] - 0s 1ms/step - loss: -91056624.0000 - accuracy: 0.0029\n",
      "Epoch 7/10\n",
      "363/363 [==============================] - 0s 1ms/step - loss: -143039296.0000 - accuracy: 0.0029\n",
      "Epoch 8/10\n",
      "363/363 [==============================] - 0s 1ms/step - loss: -209329280.0000 - accuracy: 0.0029\n",
      "Epoch 9/10\n",
      "363/363 [==============================] - 0s 1ms/step - loss: -290352768.0000 - accuracy: 0.0029\n",
      "Epoch 10/10\n",
      "363/363 [==============================] - 0s 1ms/step - loss: -386590016.0000 - accuracy: 0.0029\n",
      "Epoch 1/10\n",
      "363/363 [==============================] - 1s 2ms/step - loss: -171794.4062 - accuracy: 0.0029\n",
      "Epoch 2/10\n",
      "363/363 [==============================] - 1s 2ms/step - loss: -2900629.2500 - accuracy: 0.0029\n",
      "Epoch 3/10\n",
      "363/363 [==============================] - 1s 2ms/step - loss: -12879527.0000 - accuracy: 0.0029\n",
      "Epoch 4/10\n",
      "363/363 [==============================] - 1s 1ms/step - loss: -33793264.0000 - accuracy: 0.0029\n",
      "Epoch 5/10\n",
      "363/363 [==============================] - 1s 2ms/step - loss: -68137648.0000 - accuracy: 0.0029\n",
      "Epoch 6/10\n",
      "363/363 [==============================] - 1s 2ms/step - loss: -118273808.0000 - accuracy: 0.0029\n",
      "Epoch 7/10\n",
      "363/363 [==============================] - 1s 2ms/step - loss: -185792128.0000 - accuracy: 0.0029\n",
      "Epoch 8/10\n",
      "363/363 [==============================] - 1s 2ms/step - loss: -270003456.0000 - accuracy: 0.0029\n",
      "Epoch 9/10\n",
      "363/363 [==============================] - 1s 1ms/step - loss: -373529408.0000 - accuracy: 0.0029\n",
      "Epoch 10/10\n",
      "363/363 [==============================] - 1s 2ms/step - loss: -493106592.0000 - accuracy: 0.0029\n",
      "162/162 [==============================] - 0s 1ms/step - loss: -432267072.0000 - accuracy: 0.0021\n",
      "162/162 [==============================] - 0s 1ms/step - loss: -550695616.0000 - accuracy: 0.0021\n",
      "Model without Dropout - Loss: -432267072.0  Accuracy: 0.0021317829377949238\n",
      "Model with Dropout - Loss: -550695616.0  Accuracy: 0.0021317829377949238\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "\n",
    "\n",
    "# Define the model architecture\n",
    "model_without_dropout = keras.Sequential([\n",
    "    layers.Dense(64, activation='relu'),\n",
    "    layers.Dense(64, activation='relu'),\n",
    "    layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model_with_dropout = keras.Sequential([\n",
    "    layers.Dense(64, activation='relu'),\n",
    "    layers.Dropout(0.5),  # Adding Dropout layer\n",
    "    layers.Dense(64, activation='relu'),\n",
    "    layers.Dropout(0.5),  # Adding Dropout layer\n",
    "    layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# Compile the models\n",
    "model_without_dropout.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "model_with_dropout.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the models\n",
    "model_without_dropout.fit(x_train, y_train, epochs=10, batch_size=32)\n",
    "model_with_dropout.fit(x_train, y_train, epochs=10, batch_size=32)\n",
    "\n",
    "# Evaluate the models\n",
    "loss_without_dropout, accuracy_without_dropout = model_without_dropout.evaluate(x_test, y_test)\n",
    "loss_with_dropout, accuracy_with_dropout = model_with_dropout.evaluate(x_test, y_test)\n",
    "\n",
    "print(\"Model without Dropout - Loss:\", loss_without_dropout, \" Accuracy:\", accuracy_without_dropout)\n",
    "print(\"Model with Dropout - Loss:\", loss_with_dropout, \" Accuracy:\", accuracy_with_dropout)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "21964b10-8d33-47fa-8814-b080666e395e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d294688b-0748-46f4-8fca-6a655e4cfcc5",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "unterminated string literal (detected at line 5) (4003864290.py, line 5)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn [10], line 5\u001b[0;36m\u001b[0m\n\u001b[0;31m    2. **Interpretability**: Interpretability refers to the ability to understand and explain the model's predictions. Some regularization techniques, such as L1 regularization (Lasso), induce sparsity and can effectively perform feature selection. These techniques can lead to more interpretable models as they highlight the most important features. If interpretability is crucial, techniques like L1 regularization might be preferred.\u001b[0m\n\u001b[0m                                                                                                       ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m unterminated string literal (detected at line 5)\n"
     ]
    }
   ],
   "source": [
    "When choosing the appropriate regularization technique for a deep learning task, several considerations and tradeoffs need to be taken into account. Here are some important factors to consider:\n",
    "\n",
    "1. **Problem Complexity**: The complexity of the problem at hand plays a significant role in selecting the appropriate regularization technique. For simpler problems, a simpler regularization technique may suffice, while more complex problems may require more sophisticated techniques. Consider the nature of the problem, the size of the dataset, and the complexity of the underlying patterns when choosing a regularization technique.\n",
    "\n",
    "2. **Interpretability**: Interpretability refers to the ability to understand and explain the model's predictions. Some regularization techniques, such as L1 regularization (Lasso), induce sparsity and can effectively perform feature selection. These techniques can lead to more interpretable models as they highlight the most important features. If interpretability is crucial, techniques like L1 regularization might be preferred.\n",
    "\n",
    "3. **Impact on Model Complexity**: Regularization techniques influence the complexity of the model by adding constraints or penalties to the parameters. Some techniques, like L1 regularization, encourage sparsity and result in simpler models with fewer features. Others, like L2 regularization, shrink the magnitude of the weights without necessarily eliminating them. Consider how the choice of regularization technique affects the complexity of the model and whether it aligns with the desired tradeoff between model complexity and generalization.\n",
    "\n",
    "4. **Training Data Size**: The size of the training dataset is an important consideration. When dealing with a small dataset, regularization becomes more critical as the model may have a higher tendency to overfit. Techniques like Dropout, which introduce noise and reduce overfitting, can be beneficial in such cases. Conversely, with a large dataset, the impact of overfitting might be less pronounced, and milder regularization techniques, such as L2 regularization, could be sufficient.\n",
    "\n",
    "5. **Computational Efficiency**: Different regularization techniques have different computational requirements. Some techniques, like Dropout, require additional computations during training, which can impact the training time. Consider the computational efficiency of the chosen regularization technique, especially if you are working with large datasets or complex models.\n",
    "\n",
    "6. **Baseline Performance**: It is essential to evaluate the baseline performance of the model without regularization and understand its level of overfitting. If the baseline model already exhibits good generalization performance, you might not need to apply aggressive regularization. On the other hand, if the baseline model overfits significantly, stronger regularization techniques might be necessary.\n",
    "\n",
    "7. **Domain Knowledge**: Consider the specific characteristics of the problem domain and prior knowledge about the data. Certain regularization techniques might align better with the underlying assumptions of the data or specific domain knowledge. For example, if you know that your data has strong correlations among features, L2 regularization might be a more appropriate choice.\n",
    "\n",
    "It is important to note that the choice of regularization technique is not fixed and might require experimentation and iteration. Different techniques may yield varying results depending on the specific problem and dataset. It is recommended to try different techniques, evaluate their impact on model performance, and select the one that provides the best balance between reducing overfitting and improving generalization for your specific task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c66a5123-fc15-4f6a-bd93-030d2c346027",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "696442e5-08e2-44bf-a3bd-469c8c7c0322",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da6c586a-66b4-4286-a160-1eb667e7663f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db8ec50f-2755-4d39-8610-83693753f610",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22a3a669-68eb-4275-a76f-f8bafddfcfe0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "419a5b62-1442-47b5-87db-6eb597f9bd00",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa43b9aa-5e94-4d30-ad1a-d834c6378927",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7bd639a-fc60-4049-922c-fe5443a22faf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31782585-3c0f-4929-8003-dea972e020d8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93552213-1957-45b3-b0db-41ba234a6c5d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73271210-1176-4595-a820-fcdf9f73ef01",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a40b0bc2-a224-4c07-9e5f-7bcd42ca17cb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f14c2fcd-6a95-48f5-b297-fced5736d8fa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c3f048f-d2df-4901-8d4d-f6b5814eb1a5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f7fb3e9-42a6-4c28-9852-103278d67552",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f45856a-77d4-42e9-846d-d202e1702304",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f330762e-0a3e-48b3-a6fe-2acf09285611",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2303d42-2a60-4a30-9d5f-eebe0b289dcd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7684190d-1742-46d6-b204-d8b47efe52d7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2d85fdf-48b2-4492-b73e-0ba6000397e2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14ae151a-a605-45a4-8dd2-de10519f365f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7a70944-4464-4663-a359-e3c7155867b4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bca52051-06fb-4e77-a876-d2e3815dc4be",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6ec38cd-5d4d-431c-8815-bcf4f4b391bb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
